---
title: "Introduction to using NIMBLE models"
author: "Perry de Valpine"
date: "September 2019"
output:
  slidy_presentation: default
  beamer_presentation: default
---
<style>
slides > slide {
  overflow-x: auto !important;
  overflow-y: auto !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)
library(nimble)
library(igraph)
```

Models as graphs
=====

- Ecologists and many statisticians speak of "hierarchical models".
- Computer scientists and others sometimes speak of "graphical models".
- A hierarchical model is typically a directed acyclic graph (DAG).

NIMBLE models as objects
=====
When you create a NIMBLE model, it is an object in R.

You can:

- Get or set parameter or data values.
- Determine graph relationships.
- Calculate log probabilities.
- Simulate (draw) from distributions.
- More.

Linear regression example
=====
Let's use a really simple model:

- Linear regression with 4 data points.

```{r}
set.seed(2468)
mc <- nimbleCode({
    intercept ~ dnorm(0, sd = 1000)
    slope ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 100)
    for(i in 1:4) {
        predicted.y[i] <- intercept + slope * x[i]
        y[i] ~ dnorm(predicted.y[i], sd = sigma)
    }
})
model <- nimbleModel(mc, 
                     data = list(y = rnorm(4)),
                     inits = list(intercept = 0.5, 
                                  slope = 0.2, 
                                  sigma = 1,
                                  x = c(0.1, 0.2, 0.3, 0.4)))
```

Draw the graph:
=====
This was done with package `igraph`.
```{r echo = FALSE}

layout <- matrix(ncol = 2, byrow = TRUE,
   # These seem to be rescaled to fit in the plot area,
   # so I'll just use 0-100 as the scale
                 data = c(33, 100,
                          66, 100,
                          50, 0, # first three are parameters
                          15, 50, 35, 50, 55, 50, 75, 50, # x's
                          20, 75, 40, 75, 60, 75, 80, 75, # predicted.y's
                          25, 25, 45, 25, 65, 25, 85, 25) # y's
                 )

sizes <- c(45, 30, 30,
           rep(20, 4),
           rep(50, 4),
           rep(20, 4))

edge.color <- "black"
    # c(
    # rep("green", 8),
    # rep("red", 4),
    # rep("blue", 4),
    # rep("purple", 4))
stoch.color <- "deepskyblue2"
det.color <- "orchid3"
rhs.color <- "gray73"
fill.color <- c(
    rep(stoch.color, 3),
    rep(rhs.color, 4),
    rep(det.color, 4),
    rep(stoch.color, 4)
)


plot(model$graph, vertex.shape = "crectangle",
     vertex.size = sizes,
     vertex.size2 = 20,
     layout = layout,
     vertex.label.cex = 1.0,
     vertex.color = fill.color,
     edge.width = 3,
     asp = 0.5,
     edge.color = edge.color)
```

- Think of each line of BUGS language code as declaring one or mode *nodes*.

Get and set values
=====
This is done in natural R syntax.
```{r}
model$sigma
model$x
model$x[3] <- 0.6
model$x
```

This can be done with a compiled model too.

You can even get and set data values
=====
```{r}
model$y
model$y[1] <- 0.8
model$y
```

Get names of nodes in the graph
=====
```{r}
model$getNodeNames()
```

## Get types of nodes
```{r}
model$getNodeNames(dataOnly = TRUE)
```

```{r}
model$getNodeNames(determOnly = TRUE)
```

```{r}
model$isData('y')
```

Get node relationships
=====
```{r}
model$getDependencies("x[2]")
```

```{r}
model$getDependencies("sigma")
```

```{r}
model$getDependencies("slope")
```

Why do node relationships matter?
=====
For typical MCMC samplers, `model$getDependencies('beta[1]')` returns the nodes that need to be calculated when sampling `beta[1]`.

Results from `model$getDependencies` are in *topologically sorted* order:

- If you calculate them in order, you'll get correct results.
- E.g `predicted.y[2]` comes before `y[2]`.

Nodes vs. variables
=====
In NIMBLE:

- A variable is an object that may contain multiple nodes.  

    - `y` is a variable.

- A node is a part of a variable declared in one line of BUGS code.

    - `y[1]` ... `y[4]` are scalar nodes.

How vectorizing changes nodes
=====
```{r}
mc2 <- nimbleCode({
    intercept ~ dnorm(0, sd = 1000)
    slope ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 100)
    predicted.y[1:4] <- intercept + slope * x[1:4] #vectorized
    for(i in 1:4) {
        y[i] ~ dnorm(predicted.y[i], sd = sigma)
    }
})
model2 <- nimbleModel(mc2, 
                      data = list(y = rnorm(4)),
                      inits = list(intercept = 0.5, 
                                  slope = 0.2, 
                                  sigma = 1,
                                  x = c(0.1, 0.2, 0.3, 0.4)))
```

## Look at nodes in the vectorized model:
```{r}
model2$getNodeNames()
```

```{r}
model2$getDependencies('x[2]')
```

In this case, if `x[2]` had a prior and was being sampled in MCMC, it would be inefficient to calculate all of `y[1:4]`.  

Log probability calculations
=====
```{r}
model$calculate('y[1:4]')
```

This is the sum of log probabilities of all stochastic nodes in the calculation.

- Deterministic nodes have their values calculated but contribute 0 to the log probability.

```{r}
model$calculate( model$getDependencies('intercept'))
```

In this case, this is the sum of log probabilities from almost the entire model.

Only the priors for `slope` and `sigma` are not included.

Simulating from the model
=====

In this model, there are no random effects.  The only stochastic nodes are parameters with priors or data.

```{r}
model$sigma
model$simulate('sigma')
model$sigma
```

Data values are protected from simulation unless you are sure.

(The following is not good model-generic programming.  See below.)

```{r}
model$y
model$simulate('y') ## Will not over-write data nodes
model$y
model$simulate('y', includeData = TRUE) ## will over-write data nodes
model$y
```

Understanding *lifted nodes*
=====
Consider the following version of our linear regression model.

There is no `predicted.y[i]`.  The expression from that is directly in the `dnorm` for `y[i]`.
```{r}
mc3 <- nimbleCode({
    intercept ~ dnorm(0, sd = 1000)
    slope ~ dnorm(0, sd = 1000)
    sigma ~ dunif(0, 100)
    for(i in 1:4) {
        y[i] ~ dnorm(intercept + slope * x[i], sd = sigma)
    }
})
model3 <- nimbleModel(mc3, 
                      data = list(y = rnorm(4)),
                      inits = list(intercept = 0.5, 
                                  slope = 0.2, 
                                  sigma = 1,
                                  x = c(0.1, 0.2, 0.3, 0.4)))
```

## Look at the nodes
```{r}
model3$getNodeNames()
```

NIMBLE has created nodes in the role of `predicted.y[i]`.

These are called *lifted nodes*.  There were created by lifting an expression out of a parameter for a distribution and creating a deterministic node for that expression.


Model-generic programming
=====

Say we want a function that simulates all parts of a model that depend on some input nodes and then returns the corresponding summed log probability.  I will call this part of the model "downstream".

```{r, generic-simulate}
simulate_downstream <- function(model, nodes) {
  downstream_nodes <- model$getDependencies(nodes, downstream = TRUE)
  model$simulate( downstream_nodes, includeData = TRUE )
  logProb <- model$calculate( downstream_nodes )
  logProb
}
```
Notice that this function will work with *any* model and *any* set of input nodes.


```{r}
model$y
simulate_downstream(model, 'sigma')
model$y
```

In this case, the model doesn't have much hierarchical structure.

Always use graph structure in model-generic programming
=====

You may not know where there are lifted nodes.

Always determine what to `calculate` or `simulate` from `getDependencies` or other such tools.